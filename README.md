# ğŸ“š PDF-QA System with Mistral-Nemo

A high-performance RAG (Retrieval-Augmented Generation) system that allows users to chat with PDF documents. This project leverages **Mistral-Nemo** hosted on Kaggle's T4 GPUs and a local **Streamlit** frontend, connected via an **ngrok** tunnel.

## ğŸš€ Key Features
* **Manual RAG Implementation:** Built without LangChain to optimize performance and control the embedding/retrieval logic.
* **GPU-Powered Inference:** Uses Mistral-Nemo model for high-quality reasoning.
* **Smart Context Management:** Implemented history truncation and context cleaning to keep responses fast.
* **Persistent Indexing:** Only re-indexes when a new file is uploaded using file hashing.

## ğŸ—ï¸ Architecture
The system follows a hybrid cloud-local architecture:
1.  **Backend (Kaggle):** Runs a FastAPI server, manages the FAISS vector database, and executes LLM inference.
2.  **Frontend (Local):** A Streamlit application that handles file uploads and communicates with the backend via an ngrok tunnel.


## ğŸ› ï¸ Tech Stack
* **LLM:** Mistral-Nemo-Instruct-2407
* **Vector DB:** FAISS (Facebook AI Similarity Search)
* **Embeddings:** Sentence-Transformers (`all-MiniLM-L6-v2`)
* **Server:** FastAPI & Uvicorn
* **Tunneling:** Pyngrok
* **Frontend:** Streamlit

## ğŸ“‹ Installation & Setup

### 1. Backend (Kaggle)
1.  Open the notebook `mistral_rag.ipynb` in Kaggle.
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3.  Add your `NGROK_TOKEN` and your `API_KEY` in the settings cell.
4.  Run all cells. Copy the **Public URL** generated by ngrok (e.g., `https://...ngrok-free.dev`).

### 2. Frontend (Local)
1.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
2.  Update the `BACKEND_URL` in `app.py` with your ngrok URL.
3.  Run the app:
    ```bash
    streamlit run app.py
    ```
